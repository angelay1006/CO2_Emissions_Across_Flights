{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/flight_data.csv')\n",
    "\n",
    "## Splitting\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['aircraft_type'].fillna('Unknown', inplace=True)\n",
    "# drop columns that are closely related to the target variable\n",
    "df.drop(columns=['co2_percentage'], inplace=True)\n",
    "df.drop(columns=['avg_co2_emission_for_this_route'], inplace=True)\n",
    "# since all flights in dataset are now single-leg, the stops feature is now redundant\n",
    "df.drop(columns=['stops'], inplace=True)\n",
    "# since all currency in dataset is in USD, this feature is also redundant\n",
    "df.drop(columns=['currency'], inplace=True)\n",
    "\n",
    "y = df['co2_emissions']\n",
    "df.drop(columns=['co2_emissions'],inplace=True)\n",
    "X = df\n",
    "groups = df['aircraft_type']\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# first split - 20% to test, 80% to other\n",
    "for train_idx, test_idx in splitter.split(X,y,groups):\n",
    "    X_other, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_other, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    groups_other = groups.iloc[train_idx]\n",
    "\n",
    "# second split - 60% training, 20% validation\n",
    "X_train, X_CV, y_train, y_CV, groups_train, groups_CV = train_test_split(\\\n",
    "    X_other, y_other, groups_other, train_size=0.75, random_state=42)\n",
    "\n",
    "# collect the various features - notice we don't have ordinal features anymore\n",
    "cat_ftrs = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'aircraft_type'\\\n",
    "           , 'airline_number', 'airline_name', 'flight_number']\n",
    "num_ftrs = ['departure_time', 'arrival_time', 'duration', 'price', 'scan_date']\n",
    "\n",
    "# first, need to convert timestamps to float\n",
    "def convert_timestamps(df, timestamp_columns):\n",
    "    '''\n",
    "    converts each timestamp to a total number of minutes since midnight\n",
    "    '''\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "            converted_col_name = col + '_converted'\n",
    "            df[converted_col_name] = df[col].dt.hour * 60 + df[col].dt.minute\n",
    "            df[col] = df[converted_col_name]\n",
    "            df.drop(columns=[converted_col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "timestamp_columns = ['departure_time', 'arrival_time', 'scan_date']\n",
    "\n",
    "X_train = convert_timestamps(X_train.copy(), timestamp_columns)\n",
    "X_CV = convert_timestamps(X_CV.copy(), timestamp_columns)\n",
    "X_test = convert_timestamps(X_test.copy(), timestamp_columns)\n",
    "\n",
    "# collect the various features - notice we don't have ordinal features anymore\n",
    "cat_ftrs = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'aircraft_type'\\\n",
    "           , 'airline_number', 'airline_name', 'flight_number']\n",
    "num_ftrs = ['departure_time', 'arrival_time', 'duration', 'price', 'scan_date']\n",
    "\n",
    "# first, need to convert timestamps to float\n",
    "def convert_timestamps(df, timestamp_columns):\n",
    "    '''\n",
    "    converts each timestamp to a total number of minutes since midnight\n",
    "    '''\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "            converted_col_name = col + '_converted'\n",
    "            df[converted_col_name] = df[col].dt.hour * 60 + df[col].dt.minute\n",
    "            df[col] = df[converted_col_name]\n",
    "            df.drop(columns=[converted_col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "timestamp_columns = ['departure_time', 'arrival_time', 'scan_date']\n",
    "\n",
    "X_train = convert_timestamps(X_train.copy(), timestamp_columns)\n",
    "X_CV = convert_timestamps(X_CV.copy(), timestamp_columns)\n",
    "X_test = convert_timestamps(X_test.copy(), timestamp_columns)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/flight_data.csv')\n",
    "\n",
    "## Splitting\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['aircraft_type'].fillna('Unknown', inplace=True)\n",
    "# drop columns that are closely related to the target variable\n",
    "df.drop(columns=['co2_percentage'], inplace=True)\n",
    "df.drop(columns=['avg_co2_emission_for_this_route'], inplace=True)\n",
    "# since all flights in dataset are now single-leg, the stops feature is now redundant\n",
    "df.drop(columns=['stops'], inplace=True)\n",
    "# since all currency in dataset is in USD, this feature is also redundant\n",
    "df.drop(columns=['currency'], inplace=True)\n",
    "\n",
    "y = df['co2_emissions']\n",
    "df.drop(columns=['co2_emissions'],inplace=True)\n",
    "X = df\n",
    "groups = df['aircraft_type']\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# first split - 20% to test, 80% to other\n",
    "for train_idx, test_idx in splitter.split(X,y,groups):\n",
    "    X_other, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_other, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    groups_other = groups.iloc[train_idx]\n",
    "\n",
    "# second split - 60% training, 20% validation\n",
    "X_train, X_CV, y_train, y_CV, groups_train, groups_CV = train_test_split(\\\n",
    "    X_other, y_other, groups_other, train_size=0.75, random_state=42)\n",
    "\n",
    "# collect the various features - notice we don't have ordinal features anymore\n",
    "cat_ftrs = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'aircraft_type'\\\n",
    "           , 'airline_number', 'airline_name', 'flight_number']\n",
    "num_ftrs = ['departure_time', 'arrival_time', 'duration', 'price', 'scan_date']\n",
    "\n",
    "# first, need to convert timestamps to float\n",
    "def convert_timestamps(df, timestamp_columns):\n",
    "    '''\n",
    "    converts each timestamp to a total number of minutes since midnight\n",
    "    '''\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "            converted_col_name = col + '_converted'\n",
    "            df[converted_col_name] = df[col].dt.hour * 60 + df[col].dt.minute\n",
    "            df[col] = df[converted_col_name]\n",
    "            df.drop(columns=[converted_col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "timestamp_columns = ['departure_time', 'arrival_time', 'scan_date']\n",
    "\n",
    "X_train = convert_timestamps(X_train.copy(), timestamp_columns)\n",
    "X_CV = convert_timestamps(X_CV.copy(), timestamp_columns)\n",
    "X_test = convert_timestamps(X_test.copy(), timestamp_columns)\n",
    "\n",
    "# collect the various features - notice we don't have ordinal features anymore\n",
    "cat_ftrs = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'aircraft_type'\\\n",
    "           , 'airline_number', 'airline_name', 'flight_number']\n",
    "num_ftrs = ['departure_time', 'arrival_time', 'duration', 'price', 'scan_date']\n",
    "\n",
    "# first, need to convert timestamps to float\n",
    "def convert_timestamps(df, timestamp_columns):\n",
    "    '''\n",
    "    converts each timestamp to a total number of minutes since midnight\n",
    "    '''\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "            converted_col_name = col + '_converted'\n",
    "            df[converted_col_name] = df[col].dt.hour * 60 + df[col].dt.minute\n",
    "            df[col] = df[converted_col_name]\n",
    "            df.drop(columns=[converted_col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "timestamp_columns = ['departure_time', 'arrival_time', 'scan_date']\n",
    "\n",
    "X_train = convert_timestamps(X_train.copy(), timestamp_columns)\n",
    "X_CV = convert_timestamps(X_CV.copy(), timestamp_columns)\n",
    "X_test = convert_timestamps(X_test.copy(), timestamp_columns)\n",
    "\n",
    "# now, preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='Unknown')),\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs)])\n",
    "\n",
    "# fit_transform \n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "feature_names = preprocessor.get_feature_names_out() # Collect feature names\n",
    "\n",
    "df_train = pd.DataFrame(data=X_train_prep, columns=feature_names)\n",
    "\n",
    "# transform the CV\n",
    "df_CV = preprocessor.transform(X_CV)\n",
    "df_CV = pd.DataFrame(data=df_CV, columns=feature_names)\n",
    "\n",
    "# transform the test\n",
    "df_test = preprocessor.transform(X_test)\n",
    "df_test = pd.DataFrame(data=df_test, columns=feature_names)\n",
    "\n",
    "\n",
    "# Apply multivariate imputation \n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "n_imputations = 5\n",
    "imputed_datasets = []\n",
    "\n",
    "for i in range(n_imputations):\n",
    "    imputer = IterativeImputer(estimator=LinearRegression(), random_state=i)\n",
    "    X_train_impute = imputer.fit_transform(df_train)\n",
    "    df_train_imp = pd.DataFrame(data=X_train_impute, columns=df_train.columns)\n",
    "\n",
    "    df_CV_imp = pd.DataFrame(data=imputer.transform(df_CV), columns=df_train.columns)\n",
    "    imputed_datasets.append((df_train_imp, df_CV_imp))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
