{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./data/flight_data.csv')\n",
    "\n",
    "## Splitting\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['aircraft_type'].fillna('Unknown', inplace=True)\n",
    "# drop columns that are closely related to the target variable\n",
    "df.drop(columns=['co2_percentage'], inplace=True)\n",
    "df.drop(columns=['avg_co2_emission_for_this_route'], inplace=True)\n",
    "# since all flights in dataset are now single-leg, the stops feature is now redundant\n",
    "df.drop(columns=['stops'], inplace=True)\n",
    "# since all currency in dataset is in USD, this feature is also redundant\n",
    "df.drop(columns=['currency'], inplace=True)\n",
    "\n",
    "y = df['co2_emissions']\n",
    "df.drop(columns=['co2_emissions'],inplace=True)\n",
    "X = df\n",
    "groups = df['aircraft_type']\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# first split - 20% to test, 80% to other\n",
    "for train_idx, test_idx in splitter.split(X,y,groups):\n",
    "    X_other, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_other, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    groups_other = groups.iloc[train_idx]\n",
    "\n",
    "# second split - 60% training, 20% validation\n",
    "X_train, X_CV, y_train, y_CV, groups_train, groups_CV = train_test_split(\\\n",
    "    X_other, y_other, groups_other, train_size=0.75, random_state=42)\n",
    "\n",
    "# collect the various features - notice we don't have ordinal features anymore\n",
    "cat_ftrs = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'aircraft_type'\\\n",
    "           , 'airline_number', 'airline_name', 'flight_number']\n",
    "num_ftrs = ['departure_time', 'arrival_time', 'duration', 'price', 'scan_date']\n",
    "\n",
    "# first, need to convert timestamps to float\n",
    "def convert_timestamps(df, timestamp_columns):\n",
    "    '''\n",
    "    converts each timestamp to a total number of minutes since midnight\n",
    "    '''\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "            converted_col_name = col + '_converted'\n",
    "            df[converted_col_name] = df[col].dt.hour * 60 + df[col].dt.minute\n",
    "            df[col] = df[converted_col_name]\n",
    "            df.drop(columns=[converted_col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "timestamp_columns = ['departure_time', 'arrival_time', 'scan_date']\n",
    "\n",
    "X_train = convert_timestamps(X_train.copy(), timestamp_columns)\n",
    "X_CV = convert_timestamps(X_CV.copy(), timestamp_columns)\n",
    "X_test = convert_timestamps(X_test.copy(), timestamp_columns)\n",
    "\n",
    "# collect the various features - notice we don't have ordinal features anymore\n",
    "cat_ftrs = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'aircraft_type'\\\n",
    "           , 'airline_number', 'airline_name', 'flight_number']\n",
    "num_ftrs = ['departure_time', 'arrival_time', 'duration', 'price', 'scan_date']\n",
    "\n",
    "# first, need to convert timestamps to float\n",
    "def convert_timestamps(df, timestamp_columns):\n",
    "    '''\n",
    "    converts each timestamp to a total number of minutes since midnight\n",
    "    '''\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "            converted_col_name = col + '_converted'\n",
    "            df[converted_col_name] = df[col].dt.hour * 60 + df[col].dt.minute\n",
    "            df[col] = df[converted_col_name]\n",
    "            df.drop(columns=[converted_col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "timestamp_columns = ['departure_time', 'arrival_time', 'scan_date']\n",
    "\n",
    "X_train = convert_timestamps(X_train.copy(), timestamp_columns)\n",
    "X_CV = convert_timestamps(X_CV.copy(), timestamp_columns)\n",
    "X_test = convert_timestamps(X_test.copy(), timestamp_columns)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/flight_data.csv')\n",
    "\n",
    "## Splitting\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['aircraft_type'].fillna('Unknown', inplace=True)\n",
    "# drop columns that are closely related to the target variable\n",
    "df.drop(columns=['co2_percentage'], inplace=True)\n",
    "df.drop(columns=['avg_co2_emission_for_this_route'], inplace=True)\n",
    "# since all flights in dataset are now single-leg, the stops feature is now redundant\n",
    "df.drop(columns=['stops'], inplace=True)\n",
    "# since all currency in dataset is in USD, this feature is also redundant\n",
    "df.drop(columns=['currency'], inplace=True)\n",
    "\n",
    "y = df['co2_emissions']\n",
    "df.drop(columns=['co2_emissions'],inplace=True)\n",
    "X = df\n",
    "groups = df['aircraft_type']\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# first split - 20% to test, 80% to other\n",
    "for train_idx, test_idx in splitter.split(X,y,groups):\n",
    "    X_other, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_other, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    groups_other = groups.iloc[train_idx]\n",
    "\n",
    "# second split - 60% training, 20% validation\n",
    "X_train, X_CV, y_train, y_CV, groups_train, groups_CV = train_test_split(\\\n",
    "    X_other, y_other, groups_other, train_size=0.75, random_state=42)\n",
    "\n",
    "# collect the various features - notice we don't have ordinal features anymore\n",
    "cat_ftrs = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'aircraft_type'\\\n",
    "           , 'airline_number', 'airline_name', 'flight_number']\n",
    "num_ftrs = ['departure_time', 'arrival_time', 'duration', 'price', 'scan_date']\n",
    "\n",
    "# first, need to convert timestamps to float\n",
    "def convert_timestamps(df, timestamp_columns):\n",
    "    '''\n",
    "    converts each timestamp to a total number of minutes since midnight\n",
    "    '''\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "            converted_col_name = col + '_converted'\n",
    "            df[converted_col_name] = df[col].dt.hour * 60 + df[col].dt.minute\n",
    "            df[col] = df[converted_col_name]\n",
    "            df.drop(columns=[converted_col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "timestamp_columns = ['departure_time', 'arrival_time', 'scan_date']\n",
    "\n",
    "X_train = convert_timestamps(X_train.copy(), timestamp_columns)\n",
    "X_CV = convert_timestamps(X_CV.copy(), timestamp_columns)\n",
    "X_test = convert_timestamps(X_test.copy(), timestamp_columns)\n",
    "\n",
    "# collect the various features - notice we don't have ordinal features anymore\n",
    "cat_ftrs = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'aircraft_type'\\\n",
    "           , 'airline_number', 'airline_name', 'flight_number']\n",
    "num_ftrs = ['departure_time', 'arrival_time', 'duration', 'price', 'scan_date']\n",
    "\n",
    "# first, need to convert timestamps to float\n",
    "def convert_timestamps(df, timestamp_columns):\n",
    "    '''\n",
    "    converts each timestamp to a total number of minutes since midnight\n",
    "    '''\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "            converted_col_name = col + '_converted'\n",
    "            df[converted_col_name] = df[col].dt.hour * 60 + df[col].dt.minute\n",
    "            df[col] = df[converted_col_name]\n",
    "            df.drop(columns=[converted_col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "timestamp_columns = ['departure_time', 'arrival_time', 'scan_date']\n",
    "\n",
    "X_train = convert_timestamps(X_train.copy(), timestamp_columns)\n",
    "X_CV = convert_timestamps(X_CV.copy(), timestamp_columns)\n",
    "X_test = convert_timestamps(X_test.copy(), timestamp_columns)\n",
    "\n",
    "# now, preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='Unknown')),\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs)])\n",
    "\n",
    "# fit_transform \n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "feature_names = preprocessor.get_feature_names_out() # Collect feature names\n",
    "\n",
    "df_train = pd.DataFrame(data=X_train_prep, columns=feature_names)\n",
    "\n",
    "# transform the CV\n",
    "df_CV = preprocessor.transform(X_CV)\n",
    "df_CV = pd.DataFrame(data=df_CV, columns=feature_names)\n",
    "\n",
    "# transform the test\n",
    "df_test = preprocessor.transform(X_test)\n",
    "df_test = pd.DataFrame(data=df_test, columns=feature_names)\n",
    "\n",
    "###### functions for preprocessing\n",
    "n_imputations = 5\n",
    "# Multivariate imputation function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def multivariate_imputation(X_train, y_train, X_CV, y_CV, model, n_imputations, verbose=1):\n",
    "    mv_performance_metrics = []\n",
    "\n",
    "    for i in range(n_imputations):\n",
    "        imputer = IterativeImputer(estimator=LinearRegression(), random_state=i)\n",
    "        X_train_impute = imputer.fit_transform(X_train)\n",
    "        df_train_imp = pd.DataFrame(data=X_train_impute, columns=X_train.columns)\n",
    "\n",
    "        X_CV_impute = imputer.transform(X_CV)\n",
    "        df_CV_imp = pd.DataFrame(data=X_CV_impute, columns=X_CV.columns)\n",
    "\n",
    "        # train the passed model on imputed training set\n",
    "        trained_model = model\n",
    "        trained_model.fit(df_train_imp, y_train)\n",
    "\n",
    "        # evaluate model on imputed validation set\n",
    "        y_CV_pred = trained_model.predict(df_CV_imp)\n",
    "        mse = mean_squared_error(y_CV, y_CV_pred) \n",
    "        mv_performance_metrics.append(mse)\n",
    "\n",
    "        if verbose >= 1:\n",
    "            print(f'Imputation {i+1}: MSE = {mse}')\n",
    "\n",
    "        avg_performance = sum(mv_performance_metrics) / len(mv_performance_metrics)\n",
    "        return avg_performance\n",
    "\n",
    "# Apply XGBoost - can natively handle missing values, so the model we pass in will just be\n",
    "# an XGBoost model\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgb_modeling(X_train, y_train, X_CV, y_CV, X_test, y_test, xgb_model, verbose=1):\n",
    "    # train \n",
    "    xgb_model.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_CV, y_CV)])\n",
    "    \n",
    "    # evaluate model on validation\n",
    "    y_CV_pred = xgb_model.predict(X_CV, ntree_limit=xgb_model.best_ntree_limit)\n",
    "    mse_CV = mean_squared_error(y_CV, y_CV_pred)\n",
    "\n",
    "    # evaluate model on test\n",
    "    y_test_pred = xgb_model.predict(X_test, ntree_limit=xgb_model.best_ntree_limist)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print(f'Validation MSE: {mse_CV}, Test MSE: {mse_test}')\n",
    "\n",
    "    return mse_CV, mse_test\n",
    "\n",
    "# Apply reduced pattern submodel\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def reduced_feature_model(X_train, y_train, X_CV, y_CV, X_test, y_test, model, verbose=1):\n",
    "    all_y_test_pred = pd.DataFrame() # for collecting predictions\n",
    "\n",
    "    # find the unique patterns of the missing values\n",
    "    mask = X_test.isnull()\n",
    "    unique_rows = np.array(np.unique(mask, axis=0))\n",
    "    all_y_test_pred = pd.DataFrame()\n",
    "\n",
    "    print('there are', len(unique_rows), 'unique missing value patterns')\n",
    "\n",
    "    for i in range(len(unique_rows)):\n",
    "        print('working on unique pattern', i)\n",
    "\n",
    "        # generate X_test subset that matches the unique pattern i \n",
    "        matching_rows = [np.array_equal(row, unique_rows[i]) for row in mask.values]\n",
    "        sub_X_test = X_test[matching_rows]\n",
    "        sub_y_test = y_test[matching_rows]\n",
    "\n",
    "        # choose the according reduced features for subgroups\n",
    "        selected_features = X_train.columns[~unique_rows[i]]\n",
    "        sub_X_train = X_train[selected_features].dropna()\n",
    "        sub_X_CV = X_CV[selected_features].dropna()\n",
    "\n",
    "        # get corresponding y_train and y_CV\n",
    "        sub_y_train = y_train.loc[sub_X_train.index]\n",
    "        sub_y_CV = y_CV.loc[sub_X_CV.index]\n",
    "\n",
    "        # train model on reduced feature set\n",
    "        trained_model = model\n",
    "        trained_model.fit(sub_X_train, sub_y_train)\n",
    "\n",
    "        # predict on test subset\n",
    "        sub_y_test_pred = pd.DataFrame(trained_model.predict(sub_X_test),\\\n",
    "                                       columns=['sub_y_test_pred'], index=sub_y_test.index)\n",
    "        \n",
    "        print('RMSE:', np.sqrt(mean_squared_error(sub_y_test, sub_y_test_pred)))\n",
    "\n",
    "        all_y_test_pred = pd.concat([all_y_test_pred, sub_y_test_pred])\n",
    "\n",
    "    # rank final y_test_pred according to original y_test index\n",
    "    all_y_test_pred = all_y_test_pred.sort_index()\n",
    "    y_test = y_test.sort_index()\n",
    "\n",
    "    # calculate global RMSE and R2\n",
    "    total_RMSE = np.sqrt(mean_squared_error(y_test, all_y_test_pred))\n",
    "    total_R2 = r2_score(y_test, all_y_test_pred)\n",
    "\n",
    "    return total_RMSE, total_R2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
