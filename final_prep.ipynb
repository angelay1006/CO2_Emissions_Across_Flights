{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.11.4\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.11\"):\n",
    "    print(FAIL, \"Python version 3.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"1.24.4\", 'matplotlib': \"3.7.2\",'sklearn': \"1.3.0\", \n",
    "                'pandas': \"2.0.3\",'xgboost': \"1.7.6\", 'shap': \"0.42.1\", 'seaborn': \"0.12.2\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the dataset \n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/flight_data.csv')\n",
    "rows, cols = df.shape\n",
    "print(\"number of rows:\", rows)\n",
    "print(\"number of cols:\", cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of columns\n",
    "col_data_types = df.dtypes\n",
    "col_data_types_str = col_data_types.to_string()\n",
    "print(col_data_types_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information on target variable\n",
    "The chosen target variable is the CO2 emissions of a given flight, and it's a continuous variable since it covers a wide spectrum of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mean_val = df['co2_emissions'].mean()\n",
    "median_val = df['co2_emissions'].median()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "count = int(np.sqrt(df.shape[0]))\n",
    "sns.histplot(df['co2_emissions'], bins=count, color='skyblue', kde=False)\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.axvline(mean_val, color='r', linestyle='--')\n",
    "plt.axvline(median_val, color='g', linestyle='-')\n",
    "plt.grid(True, linestyle='--')\n",
    "\n",
    "\n",
    "plt.xlabel('CO2 emissions [kg]', fontsize=14)\n",
    "plt.ylabel('Count (Log Scale)', fontsize=14)\n",
    "plt.title('Distribution of CO2 Emissions Across Global Flights', fontsize=16)\n",
    "plt.legend({'Mean': mean_val, 'Median': median_val})\n",
    "\n",
    "plt.show()\n",
    "print(\"mean:\", mean_val)\n",
    "print(\"median:\", median_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Visualizations & Insights\n",
    "First, I'm going to look at which (continuous?) variables correlate the most with the target variable by using a correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "corr_with_target = corr_matrix['co2_emissions'].sort_values(key=abs, ascending=False)\n",
    "corr_with_target = corr_with_target.drop('co2_emissions')\n",
    "corr_with_target = corr_with_target.drop('avg_co2_emission_for_this_route')\n",
    "print(corr_with_target.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variable that correlates with CO2 emissions: price.\n",
    "plt.figure()\n",
    "sns.scatterplot(data=df, x='price', y='co2_emissions', marker='x', color='skyblue', alpha=0.6, s=10)\n",
    "plt.title('Price vs. CO2 Emissions')\n",
    "plt.xlabel('Price [USD]')\n",
    "plt.ylabel('CO2 Emissions [kg]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal variable that correlates with CO2_emissions: stops\n",
    "plt.figure()\n",
    "sns.boxplot(data=df, x='stops', y='co2_emissions')\n",
    "\n",
    "plt.title('Stops vs. CO2 Emissions')\n",
    "plt.xlabel('Number of Stops')\n",
    "plt.ylabel('CO2 Emissions [kg]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plot above suggests that flights with more stops will tend to have higher emissions. This also is confirmed with the correlation coefficient that we've seen above, of approximately 0.378. The heights of the boxes show that flights with 3+ stops have a wider range of CO2 emissions compared to flights with fewer stops - could be attributed to distance, passenger load, or the type of aircraft. Furthermore, the median line shifts upwards as the number of stops increases, which confirms the hypothesis that more stops generally correlate with higher emissions. The points outside the whiskers, the outliers, are especially abundant, which could point to the fact that this variable alone isn't enough to explain the variation in emissions. There are most likely other variables (that I mentioned) that contribute to the emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variable that correlates with CO2_emissions: aircraft\n",
    "# Since there are too many aircrafts, we'll go with the top 10 frequent\n",
    "aircraft_type_counts = df['aircraft_type'].value_counts()\n",
    "top_10 = aircraft_type_counts.nlargest(10).index\n",
    "filtered_df = df[df['aircraft_type'].isin(top_10)]\n",
    "print(top_10, aircraft_type_counts.nlargest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variable: `from_country`\n",
    "country_counts = df['from_country'].value_counts()\n",
    "top_10 = country_counts.nlargest(10).index\n",
    "filtered_df = df[df['from_country'].isin(top_10)]\n",
    "\n",
    "plt.figure()\n",
    "sns.violinplot(x='co2_emissions', y='from_country', data=filtered_df, inner='quartile')\n",
    "plt.title('CO2 Emissions by Country of Departure (Top 10 Frequent)')\n",
    "plt.xlabel('CO2 Emissions [kg]')\n",
    "plt.ylabel('Country of Departure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the width of the violin indicates the density of data points at that level, we can see that Australia has the highest CO2 emissions among all the countries that appear the most frequently in the dataset. The long \"tail\" for the violins observed could be due to multiple-leg flights, longer flights, or usage of aircraft with higher emissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variable: duration\n",
    "log_duration = np.log1p(df['duration'])\n",
    "log_co2_emissions = np.log1p(df['co2_emissions'].fillna(0))\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(x=log_duration, y=log_co2_emissions, marker='x', color='skyblue', alpha=0.6, s=10)\n",
    "plt.title('Log-Transformed Flight Duration vs. CO2 Emissions')\n",
    "plt.xlabel('Log(Time [Minutes] + 1)')\n",
    "plt.ylabel('Log(CO2 Emissions [kg] + 1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note!**\n",
    "Since dropping rows is a part of EDA and I have decided previously (when splitting on `aircraft_type` for the midterm checkpoint) that solely looking at single-leg flights would be the optimal approach, I have elected to drop the rows that contain multi-leg flights from my dataset. This is because it is not possible to obtain precise per-leg information based on aircrafts. As can be seen from the entries above, the flights in the dataset involve multiple legs operated by different aircraft types, so it would be difficult to specifically isolate a specific aircraft to visualize against CO2 emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can drop the rows and find the total number of single leg flights\n",
    "def count_aircraft_types(cell):\n",
    "    # for empty rows, we make them 0 for now\n",
    "    return cell.count('|') if isinstance(cell, str) else 0\n",
    "\n",
    "aircraft_counts = df['aircraft_type'].apply(count_aircraft_types)\n",
    "df_single_aircraft = df[aircraft_counts == 0]\n",
    "remaining_rows = len(df_single_aircraft)\n",
    "print('The number of remaining rows in this new dataframe:', remaining_rows)\n",
    "\n",
    "# reassign the variable df to our new filtered df_single_aircraft for simplicity\n",
    "df = df_single_aircraft\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Splitting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideration of iid/non-iid dataset\n",
    "First we need to consider whether this dataset is iid. I believe this dataset is non-iid because it has time-based attributes like `departure_time` -- flights that are closer in time might be more similar to each other. Similarity can also be geographical, since flights leaving from the same airports might share characteristics. Most obviously, I expect groups like `price` and `duration` to be correlated. \n",
    "\n",
    "Now that we've established that it's non iid, I am deciding to split on Group ID instead of time series data for the following reasons. The non-iid nature of the data most likely comes more from operational dependencies (like similar emissions from the same airline, aircraft type, departure country) rather than temporal factors. Though it's true that there are more flights during the summer season or during specific times, the dataset is global and includes numerous entries from different timezones/hemispheres/seasons, which could eventually balance this problem out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['aircraft_type'].fillna('Unknown', inplace=True)\n",
    "# drop columns that are closely related to the target variable\n",
    "df.drop(columns=['co2_percentage'], inplace=True)\n",
    "df.drop(columns=['avg_co2_emission_for_this_route'], inplace=True)\n",
    "# since all flights in dataset are now single-leg, the stops feature is now redundant\n",
    "df.drop(columns=['stops'], inplace=True)\n",
    "# since all currency in dataset is in USD, this feature is also redundant\n",
    "df.drop(columns=['currency'], inplace=True)\n",
    "\n",
    "y = df['co2_emissions']\n",
    "df.drop(columns=['co2_emissions'],inplace=True)\n",
    "X = df\n",
    "groups = df['aircraft_type']\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# first split - 20% to test, 80% to other\n",
    "for train_idx, test_idx in splitter.split(X,y,groups):\n",
    "    X_other, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_other, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    groups_other = groups.iloc[train_idx]\n",
    "\n",
    "# second split - 60% training, 20% validation\n",
    "X_train, X_CV, y_train, y_CV, groups_train, groups_CV = train_test_split(\\\n",
    "    X_other, y_other, groups_other, train_size=0.75, random_state=42)\n",
    "\n",
    "print(len(X_train), len(X_CV), len(X_test))\n",
    "print(df.shape)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sizes = df['aircraft_type'].value_counts()\n",
    "stats = {\n",
    "    \"total groups\": len(group_sizes),\n",
    "    \"min group size\": group_sizes.min(),\n",
    "    \"max group size\": group_sizes.max(),\n",
    "    \"mean group size\": group_sizes.mean(),\n",
    "    \"median group size\": group_sizes.median()\n",
    "}\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the various features - notice we don't have ordinal features anymore\n",
    "cat_ftrs = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'aircraft_type'\\\n",
    "           , 'airline_number', 'airline_name', 'flight_number']\n",
    "num_ftrs = ['departure_time', 'arrival_time', 'duration', 'price', 'scan_date']\n",
    "\n",
    "# first, need to convert timestamps to float\n",
    "def convert_timestamps(df, timestamp_columns):\n",
    "    '''\n",
    "    converts each timestamp to a total number of minutes since midnight\n",
    "    '''\n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "            converted_col_name = col + '_converted'\n",
    "            df[converted_col_name] = df[col].dt.hour * 60 + df[col].dt.minute\n",
    "            df[col] = df[converted_col_name]\n",
    "            df.drop(columns=[converted_col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "timestamp_columns = ['departure_time', 'arrival_time', 'scan_date']\n",
    "\n",
    "X_train = convert_timestamps(X_train.copy(), timestamp_columns)\n",
    "X_CV = convert_timestamps(X_CV.copy(), timestamp_columns)\n",
    "X_test = convert_timestamps(X_test.copy(), timestamp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='Unknown')),\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs)])\n",
    "\n",
    "# fit_transform \n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "feature_names = preprocessor.get_feature_names_out() # Collect feature names\n",
    "\n",
    "df_train = pd.DataFrame(data=X_train_prep, columns=feature_names)\n",
    "\n",
    "# transform the CV\n",
    "df_CV = preprocessor.transform(X_CV)\n",
    "df_CV = pd.DataFrame(data=df_CV, columns=feature_names)\n",
    "\n",
    "# transform the test\n",
    "df_test = preprocessor.transform(X_test)\n",
    "df_test = pd.DataFrame(data=df_test, columns=feature_names)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_CV.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data dimensions:',df_train.shape)\n",
    "perc_missing_per_ftr = df_train.isnull().sum(axis=0)/df_train.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "frac_missing = sum(df_train.isnull().sum(axis=1)!=0)/df_train.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling continuous features\n",
    "According to the above output, we don't have missing values in ordinal features, so all we have to do is take care of the missing values in `price.` This feature has a relatively low percentage of missing values, which is unlikely to have a big effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to what was covered in lecture, I will cover these three methods in preprocessing, then decide on which approach is the best for my data. For multivariate imputation, I will prepare `n` different imputations and run `n` models on them. For `XGB`, I will run `n` XGB models `n` with different seeds. Finally, for `reduced-features`, I will run `n` reduced-features models with `n` different seeds. Then, I will rank the 3 methods based on how significantly different the corresponding mean scores are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imputations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate imputation model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "def multivariate_imputation(X_train, y_train, X_CV, y_CV, model, n_imputations, verbose=1):\n",
    "    mv_performance_metrics = []\n",
    "\n",
    "    for i in range(n_imputations):\n",
    "        imputer = IterativeImputer(estimator=LinearRegression(), random_state=i)\n",
    "        X_train_impute = imputer.fit_transform(X_train)\n",
    "        df_train_imp = pd.DataFrame(data=X_train_impute, columns=X_train.columns)\n",
    "\n",
    "        X_CV_impute = imputer.transform(X_CV)\n",
    "        df_CV_imp = pd.DataFrame(data=X_CV_impute, columns=X_CV.columns)\n",
    "\n",
    "        # train the passed model on imputed training set\n",
    "        trained_model = model\n",
    "        trained_model.fit(df_train_imp, y_train)\n",
    "\n",
    "        # evaluate model on imputed validation set\n",
    "        y_CV_pred = train_model.predict(df_CV_imp)\n",
    "        mse = mean_squared_error(y_CV, y_CV_pred) \n",
    "        mv_performance_metrics.append(mse)\n",
    "\n",
    "        if verbose >= 1:\n",
    "            print(f'Imputation {i+1}: MSE = {mse}')\n",
    "\n",
    "        avg_performance = sum(mv_performance_metrics) / len(mv_performance_metrics)\n",
    "        return avg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reduced pattern submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which model is best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Choosing Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Choosing ML Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Interpreting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
